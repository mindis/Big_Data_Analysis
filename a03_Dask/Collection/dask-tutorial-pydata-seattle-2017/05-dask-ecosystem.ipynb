{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask Ecosystem\n",
    "\n",
    "In this tutorial so far we've covered:\n",
    "\n",
    "- Parallelizing general computations with `dask.delayed`\n",
    "- Parallelizing tabular computations `dask.dataframe`\n",
    "- Using the different local schedulers\n",
    "- Profiling the local schedulers using the tools in `dask.diagnostics`\n",
    "- Using the distributed scheduler in `dask.distributed`\n",
    "\n",
    "This provides a solid foundation, but `dask` and the `dask` ecosystem are much larger than these topics alone.\n",
    "\n",
    "In this notebook we provide a whirlwind tour of a few things we've missed, as well as links to where to you can find out more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array\n",
    "\n",
    "`dask.array` is another dask collection found inside the core `dask` library. Just as `dask.dataframe` mirrors `pandas`, `dask.array` mirrors `numpy`. Arrays are split into multiple *chunks* along each axis. And oerations are parallelized along the *chunks*.\n",
    "\n",
    "Much of the commonly used numpy api is implemented, including:\n",
    "\n",
    "- Elementwise operations\n",
    "- Standard ufuns\n",
    "- Indexing\n",
    "- Reductions\n",
    "- Some linear algebra (`da.linalg`)\n",
    "- Fourier Transforms (`da.fft`)\n",
    "- ...\n",
    "\n",
    "Here we create a random `dask.array`, and use the `dot` operator to peform a matrix multiplication.\n",
    "\n",
    "For more information on `dask.array`, see the [documentation](http://dask.pydata.org/en/latest/array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Create a random array of shape (10000, 10000),\n",
    "# broken into 4 (5000, 5000) chunks\n",
    "x = da.random.normal(size=(10000, 10000), chunks=(5000, 5000))\n",
    "\n",
    "# Perform a matrix multiplication with the transpose,\n",
    "# and then take the mean of the result\n",
    "res = x.dot(x.T).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the resulting graph\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the result\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bag\n",
    "\n",
    "[`dask.bag`](http://dask.readthedocs.io/en/latest/bag.html) is another builtin dask collection. It implements operations like ``map``, ``filter``, ``fold``, and ``groupby`` on collections of Python objects.  It does this in parallel and in small memory using Python iterators.  It is similar to a parallel version of [`toolz`](http://toolz.readthedocs.org/) or a Pythonic version of the [PySpark RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html).\n",
    "\n",
    "Dask bag can be useful for working with non-structured data such as `json`, or for pre-processing data before transforming into a more structured collection like `dask.dataframe`.\n",
    "\n",
    "Here we read in some line-delimited json flight data as a `dask.bag`, pull out a few fields, and then convert the result to a `dask.dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import os\n",
    "\n",
    "b = db.read_text(os.path.join('data', 'flightjson', '*.json'))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first element in the bag\n",
    "b.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # ujson is faster, use it if available\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "    \n",
    "columns = ['Year', 'Month', 'DayofMonth', 'DepDelay', 'Origin']\n",
    "\n",
    "df = (b.map(json.loads)                 # Convert each json blob to a dictionary\n",
    "       .pluck(columns)                  # Pluck out only the fields we want\n",
    "       .to_dataframe(columns=columns))  # Convert the result to a dask.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for Setting up Clusters\n",
    "\n",
    "There are a number of libraries/tools in the larger ecosystem for working with the core `dask` and `distributed` libraries. Several of these are to ease setting up a `dask.distributed` cluster for certain environments. Here we list a few with links:\n",
    "\n",
    "- [`dask-ec2`](https://github.com/dask/dask-ec2)\n",
    "- [`dask-kubernetes`](https://github.com/martindurant/dask-kubernetes)\n",
    "- [`dask-yarn`](https://github.com/dask/dask-yarn)\n",
    "- [`dask-drmaa`](https://github.com/dask/dask-drmaa)\n",
    "- [`dask-marathon`](https://github.com/mrocklin/dask-marathon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "There are currently several ways dask can help with machine learning:\n",
    "\n",
    "- The `dask.distributed` [joblib backend](http://distributed.readthedocs.io/en/latest/joblib.html) for parallelizing existing Scikit-Learn code.\n",
    "- [`dask-searchcv`](http://dask-searchcv.readthedocs.io/) for parallel and distributed hyperparameter searches\n",
    "- [`dask-glm`](https://github.com/dask/dask-glm) for parallel and distributed Generalized Linear Models\n",
    "- [`dask-xgboost`](https://github.com/dask/dask-xgboost) and [`dask-tensorflow`](https://github.com/dask/dask-tensorflow) for setting up and using `xgboost` and `tensorflow` in a distributed environment.\n",
    "\n",
    "Here we give a bit more details on a few of these:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Distributed Joblib Backend\n",
    "\n",
    "This allows parallelizing [existing joblib code](https://pythonhosted.org/joblib/) across a `dask.distributed` cluster. For example, to parallelize `GridSearchCV` from Scikit-Learn:\n",
    "\n",
    "```python\n",
    "import distributed.joblib\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(...)\n",
    "\n",
    "# Parallelize the search across a cluster\n",
    "with parallel_backend('dask.distributed', 'your-scheduler-address'):\n",
    "    search.fit(X, y)\n",
    "```\n",
    "\n",
    "A few caveats:\n",
    "- Currently this only works with a few scikit-learn operations, as many Scikit-Learn estimators hard-code which joblib backend they can use. Work is being done currently to improve this.\n",
    "- This only allows computations to scale across a cluster - it doesn't help when your data won't fit on a single machine.\n",
    "- This works best when the compute:data ratio is high, as your arrays will be serialized between workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask-SearchCV\n",
    "\n",
    "This provides drop-in replacements for Scikit-Learn's `GridSearchCV` and `RandomizedSearchCV`, with the parallelism implemented using `dask` instead of `joblib`.\n",
    "\n",
    "The benefits of using this instead of the joblib backend described above are:\n",
    "\n",
    "- Ability to use remote data. The joblib backend requires that your data is on the client (your computer), whereas `dask-searchcv` allows it to be only on the cluster.\n",
    "- Works seemlessly with dask collections. You can pass `dask.array`, `dask.dataframe`, or `dask.delayed` objects to `fit`, and `dask-searchcv` does the right thing. Note that this *doesn't* make calling `fit` work better with larger datasets - the input data will still be fully realized on each worker.\n",
    "- When searching across a `sklearn.pipeline.Pipeline`, `dask-searchcv` can merge duplicate computations, avoiding expensive repeated work.\n",
    "\n",
    "Being a drop-in for scikit-learn's versions, often only an import change is needed.\n",
    "\n",
    "```python\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from dask_searchcv import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(...)\n",
    "search.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries that use Dask\n",
    "\n",
    "Dask has been integrated into a few other projects as a way to parallelize existing code. Here we briefly list just a few of them:\n",
    "\n",
    "### Xarray\n",
    "\n",
    "[`xarray`](xarray.pydata.org) is a library for working with labelled ND-arrays, with a focus on earth and climate science. It optionally uses `dask.array` as a backend for working with large out-of-core datasets. Work is ongoing to expand this to work with `dask.distributed`, allowing `xarray` to scale across a cluster.\n",
    "\n",
    "### Scikit-Image\n",
    "\n",
    "[`Scikit-Image`](https://github.com/scikit-image/scikit-image/) optionally uses `dask.array` to parallelize image filtering operations.\n",
    "\n",
    "### Apache Airflow\n",
    "\n",
    "[`Apache Airflow`](https://github.com/apache/incubator-airflow) recently added support for using `dask.distributed` as a backend.\n",
    "\n",
    "### DataShader\n",
    "\n",
    "[`datashader`](http://datashader.readthedocs.org/) is a library for plotting larger datasets. It optionally uses dask to support out-of-core and distributed datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
