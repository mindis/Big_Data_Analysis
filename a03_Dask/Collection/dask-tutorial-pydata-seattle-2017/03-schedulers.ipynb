{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Schedulers\n",
    "\n",
    "In the previous two notebooks we used `dask.delayed` and `dask.dataframe` to create computations.  By default, these ran in a local thread pool on our personal machines.  Often, this is sufficient, especially when you are bound by NumPy and Pandas routines which release the GIL and when you are using powerful workstation computers with many cores.\n",
    "\n",
    "However sometimes you may want to execute your code in processes (for Pure Python code that holds onto the GIL), in a single thread (for profiling and debugging) or across a cluster (for larger computations).\n",
    "\n",
    "In this section we first talk about changing schedulers.  Then we use the `dask.distributed` scheduler in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Schedulers\n",
    "\n",
    "Dask separates computation description (task graphs) from execution (schedulers). This allows you to write code once, and run it locally or scale it out across a cluster.\n",
    "\n",
    "Here we discuss the *local* schedulers - schedulers that run only on a single machine. The three options here are:\n",
    "\n",
    "- `dask.threaded.get         # uses a local thread pool`\n",
    "- `dask.multiprocessing.get  # uses a local process pool`\n",
    "- `dask.get                  # uses only the main thread (useful for debugging)`\n",
    "\n",
    "In each case we change the scheduler used in a few different ways:\n",
    "\n",
    "- By providing a `get=` keyword argument to `compute`:\n",
    "\n",
    "```python\n",
    "total.compute(get=dask.multiprocessing.get)\n",
    "# or \n",
    "dask.compute(a, b, get=dask.multiprocessing.get)\n",
    "```\n",
    "\n",
    "- Using `dask.set_options`:\n",
    "\n",
    "```python\n",
    "# Use multiprocessing in this block\n",
    "with dask.set_options(get=dask.multiprocessing.get):\n",
    "    total.compute()\n",
    "# Use multiprocessing globally\n",
    "dask.set_options(get=dask.multiprocessing.get)\n",
    "```\n",
    "\n",
    "Here we repeat a simple dataframe computation from the previous section using the different schedulers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask \n",
    "import dask.multiprocessing\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': object,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "\n",
    "# Maximum non-cancelled delay\n",
    "largest_delay = df[~df.Cancelled].DepDelay.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # this uses threads by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=dask.multiprocessing.get)  # this uses processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=dask.get)  # This uses a single thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the threaded and multiprocessing schedulers use the same number of workers as cores. You can change this using the `num_workers` keyword in the same way that you specified `get` above:\n",
    "\n",
    "```\n",
    "largest_delay.compute(get=dask.multiprocessing.get, num_workers=2)\n",
    "```\n",
    "\n",
    "To see how many cores you have on your computer, you can use `multiprocessing.cpu_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Questions to Consider:\n",
    "\n",
    "- How much speedup is possible for this task (hint, look at the graph).\n",
    "- Given how many cores are on this machine, how much faster could the parallel schedulers be than the single-threaded scheduler.\n",
    "- How much faster was using threads over a single thread? Why does this differ from the optimal speedup?\n",
    "- Why is the multiprocessing scheduler so much slower here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In what cases would you want to use one scheduler over another?\n",
    "\n",
    "http://dask.pydata.org/en/latest/scheduler-choice.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "*You should skip this section if you are running low on time*.\n",
    "\n",
    "The synchronous scheduler is particularly valuable for debugging and profiling.  \n",
    "\n",
    "For example, the IPython `%%prun` magic gives us profiling information about which functions take up the most time in our computation.  Try this magic on the computation above with different schedulers.  How informative is this magic when running parallel code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%prun _ = largest_delay.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%prun _ = largest_delay.compute(get=dask.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in profiling parallel execution, dask provides several [`diagnostics`](http://dask.pydata.org/en/latest/diagnostics.html) for measuring and visualizing performance. These are useful for seeing bottlenecks in the *parallel* computation, whereas the above `prun` is useful for seeing bottlenecks in individual *tasks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler, visualize\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "with Profiler() as p, ResourceProfiler(0.25) as r:\n",
    "    largest_delay.compute()\n",
    "    \n",
    "visualize([r, p]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that while tasks are running concurrently, due to GIL effects we're only achieving parallelism during early parts of `pd.read_csv` (mostly the byte operations).\n",
    "\n",
    "\n",
    "*It should be noted that the `dask.diagnostics` module is only useful when profiling on a single machine. The `dask.distributed` scheduler has its own set of diagnostics..*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Scheduler\n",
    "\n",
    "The `dask.distributed` system is composed of a single centralized scheduler and several worker processes.  We can either set these up manually as command line processes or have Dask set them up for us from the notebook.  \n",
    "\n",
    "\n",
    "#### Automatically setup a local cluster\n",
    "\n",
    "Starting a single scheduler and worker on the local machine is a common case. Dask will set up a local cluster for you if you provide no scheduler address to `Client`:\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "```\n",
    "\n",
    "If you choose this approach then there is no need to set up a `dask-scheduler` or `dask-worker` process as described below.\n",
    "\n",
    "\n",
    "#### Manually setup a distributed cluster\n",
    "\n",
    "It is good to know how to set things up manually in case you want to try out Dask on a small cluster of your own.  However, if you are unfamiliar with the command line you can safely skip this section and go down to the Automatic section below. We run the `dask-scheduler` process on one machine.\n",
    "\n",
    "```\n",
    "$ dask-scheduler\n",
    "distributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n",
    "distributed.bokeh.application - INFO - Web UI: http://127.0.0.1:8787/status/\n",
    "```\n",
    "\n",
    "The scheduler reports that it is running at 127.0.0.1 on port 8786.  The address 127.0.0.1 is another name for \"localhost\" or \"this machine\".  We will need to give this address to the workers and client so you might want to copy it now.\n",
    "\n",
    "We now run the `dask-worker` process on every machine that we want to use for computation.  For right now this is probably just once on our laptop, but in production this may be on many different machines:\n",
    "\n",
    "```\n",
    "$ dask-worker 127.0.0.1:8786\n",
    "distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45011\n",
    "distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:8786\n",
    "```\n",
    "\n",
    "*Note*: for this tutorial we want to start the dask-worker process in the `dask-tutorial-pydata-seattle/` directory.  This will ensure that the workers Python processes have access to the same data that our notebook process does.\n",
    "\n",
    "```\n",
    "$ cd dask-tutorial-pydata-seattle/  # navigate to whereever you have started your notebooks\n",
    "~/dask-tutorial-pydata-seattle/ $ dask-worker 127.0.0.1:8786\n",
    "```\n",
    "\n",
    "*Note*: By default the dask-worker command line tool starts a single process with a thread pool with as many threads as you have cores on your computer.  If you are doing mostly GIL-released computations (numpy, pandas, scikit-learn) then this is the right choice.  However if you are doing mostly GIL-bound comptutations (Python code, pandas with text, parsing) then you will want to start the workers with multiple processes and one thread per process\n",
    "\n",
    "```\n",
    "$ dask-worker 127.0.0.1:8786 --nprocs 8 --nthreads 4\n",
    "```\n",
    "    \n",
    "You can see more options by asking for help\n",
    "\n",
    "```\n",
    "$ dask-worker --help\n",
    "```\n",
    "    \n",
    "When the scheduler and workers are running you can connect to them using a Dask `Client`, giving it the same address of the scheduler that you gave to the worker.\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786')\n",
    "```\n",
    "\n",
    "### More information\n",
    "\n",
    "You can find more information at the following documentation pages:\n",
    "\n",
    "- [Quickstart](http://distributed.readthedocs.io/en/latest/quickstart.html)\n",
    "- [Setup Network](http://distributed.readthedocs.io/en/latest/setup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a local cluster\n",
    "\n",
    "As mentioned above, the multiprocessing scheduler can be inefficient for complicated workflows. The distributed scheduler doesn't have this downside, and works fine locally. This makes it often a good replacement for the multiprocessing scheduler, even when working on a single machine.\n",
    "\n",
    "Here we startup a local cluster, and use it to repeat the same dataframe computation as done above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Setup a local cluster.\n",
    "# By default this sets up 1 worker per core\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=client.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Questions to Consider\n",
    "\n",
    "- How does this compare to the optimal parallel speedup?\n",
    "- Why is this faster than the threaded scheduler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client takes over by default\n",
    "\n",
    "Actually, we didn't need to add `get=client.get`.  The distributed scheduler takes over as the default scheduler for all collections when the Client is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # This used to use threads by default, now it uses dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Distributed Cluster\n",
    "\n",
    "We've setup a distributed cluster for you to use. The scheduler address is `schedulers:9000`. Here we create a new client to connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = Client('schedulers:9000')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- How many cores does this new cluster have?\n",
    "- How many workers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the same computation as above, but on the distributed cluster. As mentioned above, we don't need to specify the `get` function - the new `Client` took over as the default scheduler when it was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # This used to use threads by default, now it uses dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happened Here?\n",
    "\n",
    "Why did this operation fail when it worked with the local cluster?\n",
    "\n",
    "When working in a *distributed* environment you need to be careful about some things. When computing locally you can be sure that your workers share the same filesystem and python libraries. This is no longer true when working distributed.\n",
    "\n",
    "In this case, the task graph contained references to a filepath that exists locally, but doesn't exist on the workers. A few common patterns when working with distributed data are:\n",
    "\n",
    "- Store data in the cloud where it can be accessed by all workers (e.g. `s3`, `hdfs`, `gcfs`)\n",
    "- Use a networed filesystem that's accessible to all nodes\n",
    "- Duplicate the files on all nodes\n",
    "\n",
    "In a future notebook we'll switch to using cloud storage. For now we'll change our example computation to use simulated flight data which doesn't rely on any files on disk. \n",
    "\n",
    "*Note: you don't need to understand this code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake flight data as a dask dataframe\n",
    "# You don't need to understand this code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import delayed\n",
    "\n",
    "@delayed(pure=True)\n",
    "def make_chunk(start, end, N, seed):\n",
    "    N = int(N)\n",
    "    rs = np.random.RandomState(seed)\n",
    "    origin = rs.choice(np.array(['EWR', 'LGA', 'JFK'], dtype='O'),\n",
    "                       size=N, p=[0.45, 0.4, 0.15])    \n",
    "    date = rs.randint(np.datetime64(start).astype('i8'),\n",
    "                      np.datetime64(end).astype('i8'),\n",
    "                      size=N).astype('M8[D]')\n",
    "    cancelled = rs.random_sample(N) < 0.01\n",
    "    date[cancelled] = None\n",
    "    origin[cancelled] = None\n",
    "    delay = np.full(N, np.NaN)\n",
    "    for o, s in [('EWR', 0.4), ('LGA', 0.42), ('JFK', 0.47)]:\n",
    "        m = origin == o\n",
    "        delay[m] = (rs.gamma(s, 40, m.sum()) - 10).astype('i8')\n",
    "    return pd.DataFrame({'Date': date,\n",
    "                         'Origin': origin,\n",
    "                         'DepDelay': delay,\n",
    "                         'Cancelled': cancelled}).sort_values('Date')\n",
    "\n",
    "chunks = [make_chunk('%d-01-01' % y, '%d-01-01' % (y + 1), 1e6, i)\n",
    "          for i, y in enumerate(range(1990, 2000))]\n",
    "df = dd.from_delayed(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_delay = df[~df.Cancelled].DepDelay.max()\n",
    "largest_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "One of the main advantages of using the distributed scheduler is the diagnostics dashboards that should be hosted [here](../9002/status).  Visit that link and then run the computation again.\n",
    "\n",
    "You may want to arrange your notebook and this webpage side-by-side on your screen so that you can see both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the following computations while looking at the diagnostics page. In each case what is taking the most time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of flights\n",
    "_ = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights\n",
    "_ = len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights per-airport\n",
    "_ = df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average departure delay from each airport?\n",
    "_ = df[~df.Cancelled].groupby('Origin').DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay per day-of-week\n",
    "_ = df.groupby(df.Date.dt.dayofweek).DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New API\n",
    "\n",
    "The distributed scheduler is more sophisticated than the single machine schedulers.  It comes with more functions to manage data, computing in the background, and more.  The distributed scheduler also has entirely separate documentation\n",
    "\n",
    "-  http://distributed.readthedocs.io/en/latest/\n",
    "-  http://distributed.readthedocs.io/en/latest/api.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
