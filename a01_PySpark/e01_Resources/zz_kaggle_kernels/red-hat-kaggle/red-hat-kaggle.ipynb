{"cells":[{"cell_type":"markdown","source":["# [Predicting Red Hat Business Value](https://www.kaggle.com/c/predicting-red-hat-business-value)\nHosted by [Kaggle](https://www.kaggle.com/)\n\n## In short we will ...\n\n1. Configurations\n   * Mount an *s3 bucket*\n   * Set the default *data* & *submission* path\n   * Set default number of partitions\n2. Import the competition data sets\n  * Download the competition data sets directly\n3. Create an *Estimator*\n  * Estimator\n  * Model\n4. Preprocess the raw data sets\n  * Load *act_train* and *people* csv file as a dataframe\n  * Parse the dates into different bins\n  * Join *people* and *act_train* dataframes\n    * Rename the column names of *people* and *train*\n    * Join and cache\n  * Create *Training*, *validation* and *Test* sets from the joined dataframe\n5. Make predictions\n  * Hyper-parameter optimization\n    * Logistic regression\n    * Random forest classifier\n  * Ensemble\n6. Create submission"],"metadata":{}},{"cell_type":"markdown","source":["## 1: Configurations"],"metadata":{}},{"cell_type":"markdown","source":["## 1.1: Mount an s3 bucket\nTo export the submission dataframe at the end, we need to mount an _[AWS s3](https://aws.amazon.com/s3/) bucket_. For detailed instructions visit the following article by _Databricks_.\n\n> [Data Import How-To Guide](https://databricks.com/wp-content/uploads/2015/08/Databricks-how-to-data-import.pdf) by _Databricks_"],"metadata":{}},{"cell_type":"code","source":["import urllib\n\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nENCODED_SECRET_KEY = urllib.quote(SECRET_KEY, \"\")\nAWS_BUCKET_NAME = \"\"\nMOUNT_NAME = \"s3\"\n\n# comment out the following line after you have successfully mounted your bucket\n# dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### 1.2: Set the default *data* & *submission* path\nIf you prefer to import the competition data sets by pushing them to your s3 bucket first:\n - Make sure to set the *data_root* to point to the directory containing the raw data sets in your s3 bucket"],"metadata":{}},{"cell_type":"code","source":["data_root = '/tmp/redhat_data/'\nsubmission_path = '/mnt/{}/submission.txt'.format(MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### 1.3: Set default number of partitions\n\nFollowing the configuration from [UC Berkeleyx cs120x](https://www.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x) lab 3."],"metadata":{}},{"cell_type":"code","source":["sqlContext.setConf('spark.sql.shuffle.partitions', '6')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Part 2: Import the competition data sets\n\nSince we have mounted an s3 bucket we can simply upload the competition data to the s3 bucket and access them in this notebook. In this case, make sure to update the **data_root** to point to the directory containing the raw competition data sets. (skip section 2.1 if you choose to follow this approach).\n\nAlternatively you can download all the competition data directly, and this what section 2.1 is about."],"metadata":{}},{"cell_type":"markdown","source":["### 2.1: Download the competition data sets directly\n\nHere we adapt the script posted by [John Ramey](http://ramhiser.com/) and the comment by [Ole Henrik SkogstrÃ¸m](https://disqus.com/by/ole_henrik_skogstr_m/) to download all the competition data sets.\n\n> [How to Download Kaggle Data with Python and requests.py](http://ramhiser.com/2012/11/23/how-to-download-kaggle-data-with-python-and-requests-dot-py/)\n\n* Due to the Requests API [changes](http://www.python-requests.org/en/master/api/#api-changes) we will change the _prefetch_ flag to _stream_"],"metadata":{}},{"cell_type":"code","source":["from requests import get, post\nfrom os import mkdir, remove\nfrom os.path import exists\nfrom shutil import rmtree\nimport zipfile\n\ndef purge_all_downloads(db_full_path):\n  # Removes all the downloaded datasets\n  if exists(db_full_path): rmtree(db_full_path)\n\ndef datasets_are_available_locally(db_full_path, datasets):\n  # Returns True only if all the competition datasets are available locally in Databricks CE\n  if not exists(db_full_path): return False\n  for df in datasets:\n    # Assumes all the datasets end with '.csv' extention\n    if not exists(db_full_path + df + '.csv'): return False\n  return True\n\ndef remove_zip_files(db_full_path, datasets):\n  for df in datasets:\n    remove(db_full_path + df + '.csv.zip')\n    \ndef unzip(db_full_path, datasets):\n  for df in datasets:\n    with zipfile.ZipFile(db_full_path + df + '.csv.zip', 'r') as zf:\n      zf.extractall(db_full_path)\n  remove_zip_files(db_full_path, datasets)\n\ndef download_datasets(competition, db_full_path, datasets, username, password):\n  # Downloads the competition datasets if not availible locally  \n  if datasets_are_available_locally(db_full_path, datasets):\n    print 'All the competition datasets have been downloaded, extraced and are ready for you !'\n    return\n  \n  purge_all_downloads(db_full_path)\n  mkdir(db_full_path)\n  kaggle_info = {'UserName': username, 'Password': password}\n  \n  for df in datasets:\n    url = (\n      'https://www.kaggle.com/account/login?ReturnUrl=' +\n      '/c/' + competition + '/download/'+ df + '.csv.zip'\n    )\n    request = post(url, data=kaggle_info, stream=True)\n    \n    # write data to local file\n    with open(db_full_path + df + '.csv.zip', \"w\") as f:\n      for chunk in request.iter_content(chunk_size = 512 * 1024):\n        if chunk: f.write(chunk)\n  \n  # extract competition data \n  unzip(db_full_path, datasets)\n  print('done !')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# KAGGLE_USERNAME = ''\n# KAGGLE_PASSWORD = ''\n\n# download_datasets(\n#   competition='predicting-red-hat-business-value',\n#   db_full_path= ('/dbfs' + data_root), # here we need the full path\n#   datasets=['act_train', 'act_test', 'sample_submission', 'people'],\n#   username=KAGGLE_USERNAME,\n#   password=KAGGLE_PASSWORD\n# )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(dbutils.fs.ls(data_root))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## 3: Create an Estimator\n\nTo transform the raw dataframes within a pipeline, here we create a custom estimator that:\n\n- Extracts one-hot-encoding (OHE) features from categorical columns\n- Adds the continuous columns to the feature vector\n- Extracts OHE features from boolean columns\n- Creates a sparse feature vector\n- Carries over other columns\n\nWe will use the answer by [zero323](http://stackoverflow.com/users/1560062/zero323) on Stackoverflow in designing a one-hot-encoding __estimator__.\n\n> [how to roll a custom estimator in pyspark mllib](http://stackoverflow.com/questions/37270446/how-to-roll-a-custom-estimator-in-pyspark-mllib)\n\nThe OHE dictionary was inspired by UC Berkeleyx cs120x lab-3.\n\n> [UC Berkeleyx cs120x](https://www.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x)"],"metadata":{}},{"cell_type":"markdown","source":["### 3.1: Estimator\n\nThe Encoder creates a dictionary mapping (key, value) tuples (where each key is a __column name__ and each values is a __category__) to a unique integer. This dictionary is then used to create a __Model__."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Estimator\nfrom collections import defaultdict\nfrom pyspark.sql.functions import explode\n\nclass Encoder(Estimator):\n  # INIT\n  _defaultParamMap = {}\n  _paramMap = {}\n  _params = {}\n  \n  def __init__(self, label_column_name='label', id_column_name='id', weight_column_name='weight'):\n    self.label_column_name = label_column_name\n    self.id_column_name = id_column_name\n    self.weight_column_name = weight_column_name\n  def set_label_col(self, name):\n    self.label_column_name = name\n  def set_id_col(self, name):\n    self.id_column_name = name\n  def set_weight_column_name(name):\n    self.weight_column_name = name\n  \n  # HELPERS\n  def create_key_value_rdd(self, input_df):\n    # returns a key value rdd where each key is the column name and each value is the value of the attribute\n    return(\n      input_df.rdd\n      .map(lambda r: r.asDict())\n      .map(lambda h: [(k, v) for k, v in h.iteritems()])\n    )\n  \n  def create_ohe_dict(self, input_rdd):\n    # creates a dict to mapping each key value pair to a unique number\n    return (\n      input_rdd\n      .flatMap(lambda l: l)\n      .distinct()\n      .zipWithIndex()\n      .collectAsMap()\n    )\n  \n  def group_columns_by_type(self, schema):\n    column_name_type_tuples = map(lambda col: col.simpleString().split(':'), schema)\n    # exclude ID, Label, and Weight columns\n    feature_columns = [\n      col for col in column_name_type_tuples if col[0] not in [self.label_column_name, self.id_column_name, self.weight_column_name]\n    ]\n\n    \n    boolean_column_names = [col[0] for col in feature_columns if col[1] == 'boolean']\n    string_column_names = [col[0] for col in feature_columns if col[1] == 'string']\n    integer_column_names = [col[0] for col in feature_columns if col[1] == 'int']\n    \n    return (string_column_names, boolean_column_names, integer_column_names)\n    \n  # FITTING MODEL\n  def _fit(self, df):\n    # Group columns\n    string_column_names, boolean_column_names, integer_column_names = self.group_columns_by_type(df.schema)\n    \n    # Create an OHE dictionary for columns of String type\n    kv_rdd = self.create_key_value_rdd(df.select(*string_column_names))\n    ohe_dict = self.create_ohe_dict(kv_rdd)\n    \n    return EncoderModel(\n      ohe_dict, self.id_column_name, self.label_column_name, self.weight_column_name,\n      boolean_column_names, string_column_names, integer_column_names\n    )"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### 3.2: Model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Model\nfrom pyspark.ml.linalg import SparseVector, VectorUDT\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\nclass EncoderModel(Model):\n  # INIT\n  _defaultParamMap = {}\n  _paramMap = {}\n  _params = {}\n  \n  def __init__(\n    self, one_hot_encoding_dictionary, id_col_name, label_col_name, weight_column_name,\n    boolean_column_names, string_column_names, integer_column_names\n  ):\n    self.one_hot_encoding_dictionary = one_hot_encoding_dictionary\n    self.id_column_name = id_col_name\n    self.label_column_name = label_col_name\n    self.weight_column_name = weight_column_name\n    self.boolean_column_names = boolean_column_names\n    self.string_column_names = string_column_names\n    self.integer_column_names = integer_column_names\n    \n  # HELPERS\n  def create_key_value_rdd(self, input_df):\n    # returns a key value rdd where each key is the column name and each value is the value of the attribute\n    return(\n      input_df.rdd\n      .map(lambda r: r.asDict())\n      .map(lambda h: [(k, v) for k, v in h.iteritems()])\n    )\n  \n  def gen_feats_rdd(self, input_rdd, ohe_dict_broadcast, has_id, has_weight, has_label):\n    # HELPERS\n    def get_value(raw_feats, column_name):\n      for observation in raw_feats:\n        if observation[0] == column_name: return observation[1]\n      return None\n    def get_values(raw_feats, column_names):\n      args = []\n      for observation in raw_feats:\n        if observation[0] in column_names:\n          args.append(observation[1])\n      return args\n    def get_bool_feats(raw_feats, boolean_column_names, index):\n      values = get_values(raw_feats, boolean_column_names)\n      return zip(range(index, index + len(values)), map(lambda b: int(b), values))\n    def get_int_feats(raw_feats, integer_column_names, index):\n      values = get_values(raw_feats, integer_column_names)\n      return zip(range(index, index + len(values)), values)\n    def get_ohe_feats(raw_feats, ohe_dict_broadcast):\n      args = []\n      for observation in raw_feats:\n        if observation in ohe_dict_broadcast.value:\n          index = ohe_dict_broadcast.value[observation]\n          args.append((index, 1))\n      return args\n    \n    # FEATURE VECTOR\n    boolean_column_names = self.boolean_column_names\n    integer_column_names = self.integer_column_names\n    def join_feats(raw_feats, ohe_dict_broadcast): \n      ohe_feats = get_ohe_feats(raw_feats, ohe_dict_broadcast)\n      next_index = len(ohe_dict_broadcast.value)\n      bool_feats = get_bool_feats(raw_feats, boolean_column_names, next_index)\n      next_index += len(boolean_column_names)\n      int_feats = get_int_feats(raw_feats, integer_column_names, next_index)\n      next_index += len(integer_column_names)\n      return SparseVector(next_index, ohe_feats + bool_feats + int_feats)\n    \n    # return the new dataframe\n    id_column_name = self.id_column_name\n    weight_column_name = self.weight_column_name\n    label_column_name = self.label_column_name\n    def transform_row(f):\n      new_row = []\n      if has_id:\n        new_row.append(get_value(f, id_column_name))\n      if has_weight:\n        new_row.append(get_value(f, weight_column_name))\n      new_row.append(join_feats(f, ohe_dict_broadcast))\n      if has_label:\n        new_row.append(get_value(f, label_column_name))\n      return new_row\n    \n    return input_rdd.map(lambda f: transform_row(f))\n    \n  # TRANSFORM DATAFRAME\n  def _transform(self, input_df):\n    # creates a ohe dataframe with id, label and features columns\n    has_label = self.label_column_name in input_df.schema.names\n    has_id = self.id_column_name in input_df.schema.names\n    has_weight = self.weight_column_name in input_df.schema.names\n    \n    ohe_dict_broadcast = sc.broadcast(self.one_hot_encoding_dictionary)\n    \n    input_kv_rdd = self.create_key_value_rdd(input_df)\n    input_ohe_rdd = self.gen_feats_rdd(input_kv_rdd, ohe_dict_broadcast, has_id, has_weight, has_label)\n    \n    # for some reason I couldn't cast the Neumerical columns in toDF. I had to first create them as StringType columns and then cast them to DoubleType\n    fields = []\n    if has_id:\n      fields.append(StructField(\"id\", StringType(), True))\n    if has_weight:\n      fields.append(StructField(\"weight_st\", StringType(), True))\n    fields.append(StructField(\"features\", VectorUDT(), True))\n    if has_label:\n      fields.append(StructField(\"label_st\", StringType(), True))\n\n    schema = StructType(fields)\n    ohe_df = input_ohe_rdd.toDF(schema)\n    \n    if has_label:\n      ohe_df = ohe_df.withColumn('label', ohe_df['label_st'].cast(DoubleType())).drop(\"label_st\")\n    if has_weight:\n      ohe_df = ohe_df.withColumn('weight', ohe_df['weight_st'].cast(DoubleType())).drop(\"weight_st\")\n    return ohe_df"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Part 4: Preprocess the raw data sets"],"metadata":{}},{"cell_type":"markdown","source":["## 4.1: Load *act_train* and *people* csv file as a dataframe"],"metadata":{}},{"cell_type":"code","source":["raw_train_df = (\n  sqlContext.read.format('com.databricks.spark.csv')\n  .options(header=True, delimiter=',', inferschema=True)\n  .load(data_root+'/act_train.csv')\n).drop('activity_id')\nprint raw_train_df.count()\nprint raw_train_df.printSchema()\n\nraw_people_df = (\n  sqlContext.read.format('com.databricks.spark.csv')\n  .options(header=True, delimiter=',', inferschema=True)\n  .load(data_root+'/people.csv')\n)\nprint raw_people_df.count()\nprint raw_people_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## 4.2: Parse the dates into different bins"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StringType\n\ndef parse_dates(input_df):\n  udf_date_to_day = udf(lambda date: date.day, StringType())\n  udf_date_to_month = udf(lambda date: date.month, StringType())\n  udf_date_to_weekday = udf(lambda date: date.weekday(), StringType())\n  udf_date_to_year = udf(lambda date: date.year, StringType())\n  \n  return (\n    input_df.select(\n      '*',\n      udf_date_to_day(input_df['date']).alias('d_day'),\n      udf_date_to_month(input_df['date']).alias('d_mon'),\n      udf_date_to_weekday(input_df['date']).alias('d_wd'),\n      udf_date_to_year(input_df['date']).alias('d_yr')\n    ).drop('date')\n  )"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["raw_train_with_time_feat_df = parse_dates(raw_train_df)\nraw_people_with_time_feat_df = parse_dates(raw_people_df)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["##### K-Means (still looking into it)\n\nHere I tried to group users together with KMeans and feed these groups as new categorical features to the optimizer.\n\nThis is an expensive operation for large Ks and did not improve the AUC score much.\n\nIf you want to try this out, just uncomment the following lines and comment the code where I assign *people_renamed_df* in section 4.3.1"],"metadata":{}},{"cell_type":"code","source":["# from pyspark.ml.clustering import KMeans\n# kmeans = KMeans(maxIter=80)\n# people_vec_df = (\n#   Encoder(id_column_name='people_id')\n#   .fit(raw_people_with_time_feat_df)\n#   .transform(raw_people_with_time_feat_df)\n#   .withColumnRenamed('id', 'people_id')\n# )\n\n# people_acc_df = people_vec_df\n# for i in range(2, 4):\n#   col_name = 'p_c_{}'.format(i)\n#   kmeans.setK(i).setPredictionCol(col_name)\n#   model = kmeans.fit(people_vec_df)\n#   temp = model.transform(people_acc_df)\n#   people_acc_df = temp.withColumn(col_name+'s', temp[col_name].cast(StringType())).drop(col_name)\n# people_acc_df = people_acc_df.drop('features')\n\n# people_prep_df = raw_people_with_time_feat_df.join(people_acc_df, on='people_id', how='inner')\n\n# people_renamed_df = people_prep_df.toDF(\n#   *map(lambda col_name: 'p_' + col_name, people_prep_df.schema.names)\n# ).withColumnRenamed('p_people_id', 'p_id')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### 4.3: Join people and act_train dataframes"],"metadata":{}},{"cell_type":"markdown","source":["#### 4.3.1: Rename the column names of *people* and *train*\n\nThe column names in the people dataframe are in conflict with the train dataframe.\n* We will rename the columns before joining the two dataframes.\n\n__This name conflict will not prevent the join operation but I want to differentiate between columns from train and people dataframe. This becomes useful when we construct the *one-hot-encoding* dictionary__"],"metadata":{}},{"cell_type":"code","source":["people_renamed_df = raw_people_with_time_feat_df.toDF(\n  *map(lambda col_name: 'p_' + col_name, raw_people_with_time_feat_df.schema.names)\n).withColumnRenamed('p_people_id', 'p_id')\n\ntrain_renamed_df = (\n  raw_train_with_time_feat_df\n  .toDF(*map(lambda col_name: 't_' + col_name, raw_train_with_time_feat_df.schema.names))\n  .withColumnRenamed('t_people_id', 'p_id').withColumnRenamed('t_outcome', 'label')\n)\n\nprint people_renamed_df.schema.names\nprint train_renamed_df.schema.names"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### 4.3.2: Join, drop-duplicates and cache"],"metadata":{}},{"cell_type":"code","source":["joined_train_people_df = (\n  train_renamed_df\n  .join(people_renamed_df, on='p_id', how='inner')\n  .dropDuplicates()\n  .cache()\n)\n\nprint joined_train_people_df.count()\nprint joined_train_people_df.schema.names"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### 4.4: Create *Training*, *validation* and *Test* sets from the *joined* dataframe\n\nI have tried two different splitting strategy:\n* Random split\n  * It is fast; however, it does not provide a validation set representative of the leader board\n* Random split by people_id\n  * It better represents the leader board; however, it is excruciatingly slow !\n  \nWe will use the answer by [zero323](http://stackoverflow.com/users/1560062/zero323) on Stackoverflow to create three dataframes with disjoined set of people_ids.\n\n> [pyspark split filter dataframe by columns values](http://stackoverflow.com/questions/35190109/pyspark-split-filter-dataframe-by-columns-values)"],"metadata":{}},{"cell_type":"code","source":["# a simple random split\n\n# weights = [0.8, 0.2] # training and test weights\n# fraction = 0.01 # about 40K\n# seed = 110\n\n# train_df, test_df = joined_train_people_df.sample(False, fraction, seed).randomSplit(weights, seed)\n\n# print train_df.cache().count()\n# print test_df.cache().count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["weights = [0.8, 0.1, 0.1]\nseed = 110\n\ntrain_p_id, valid_p_id, test_p_id = map(\n  lambda df: df.rdd.flatMap(lambda id: id).collect(),\n  (\n    joined_train_people_df\n    .select('p_id')\n    .distinct()\n    .sample(False, 0.1)\n    .randomSplit(weights=weights, seed=seed)\n  )\n)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["train_dub_df = (\n  joined_train_people_df\n  .where(joined_train_people_df['p_id'].isin(train_p_id))\n  .drop('p_id')\n)\ntrain_df = train_dub_df.groupBy(train_dub_df.schema.names).count().withColumnRenamed('count', 'weight')\n\nvalid_df = joined_train_people_df.where(joined_train_people_df['p_id'].isin(valid_p_id)).drop('p_id').dropDuplicates()\ntest_df = joined_train_people_df.where(joined_train_people_df['p_id'].isin(test_p_id)).drop('p_id').dropDuplicates()\n\nprint train_df.cache().count()\nprint valid_df.cache().count()\nprint test_df.cache().count()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Part 5: Make predictions"],"metadata":{}},{"cell_type":"markdown","source":["### 5.1: Hyper-parameter optimization\n\nIf a validation set with unseen people_id is not important, we can simply use a provided TrainValidationSplit (I have commented out the code for this).\n\n> [ML Tuning](https://spark.apache.org/docs/latest/ml-tuning.html)\n\nSince we want to use a specific validation set, we should write our own optimizer (basically a couple of nested **for loops**).\n\nRunning a notebook on the **Community Edition** of Databricks has a number of implications:\n\n1. We only have 6GB of RAM to play with\n\n2. Our cluster gets terminated about every hour if the notebook is inactive.\n\nAs a result finetuning random forests on the complete dataset leads to memory errors.\nAlso our cluster gets terminated if we pick a large set of hyper parameters. This is due to the second point (the cell runs for more than an hour and our cluster gets terminated)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\nfrom pyspark.ml import Pipeline\n\nencoder = Encoder()\nroc_eval = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["encoder_model = encoder.fit(train_df)\ntrain_vec_df = encoder_model.transform(train_df)\nvalid_vec_df = encoder_model.transform(valid_df)\ntest_vec_df = encoder_model.transform(test_df)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# this is a helper for visualizing a heatmap of validation scores\n\n# import numpy as np\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# def display_heat_map(s, a, b):\n#   x, y = len(a), len(b) \n#   heat_map_matrix = np.array(s).reshape(x, y)\n#   f = plt.figure(figsize=(y + 1, x))\n#   sns.heatmap(heat_map_matrix, square=True)\n#   display(f)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["#### 5.1.1: Logistic regression"],"metadata":{}},{"cell_type":"code","source":["# from pyspark.ml.classification import LogisticRegression\n\n# lr_reg_param = [1e-6]\n# lr_max_iter = [80]\n# lr = LogisticRegression(\n#   standardization=False, threshold=0.5,\n#   predictionCol='LR_P', probabilityCol='LR_Prob', rawPredictionCol='LR_rawProb'\n# )\n# roc_eval.setRawPredictionCol('LR_rawProb')\n\n# lr_pip = Pipeline(stages=[encoder, lr])\n\n# lr_param_grid = (\n#   ParamGridBuilder()\n#   .addGrid(lr.regParam, lr_reg_param)\n#   .addGrid(lr.maxIter, lr_max_iter)\n#   .build()\n# )\n\n# lr_tv = TrainValidationSplit(\n#   estimator=lr_pip,\n#   estimatorParamMaps=lr_param_grid,\n#   evaluator=roc_eval,\n#   trainRatio=0.8\n# )\n\n# lr_model = lr_tv.fit(train_df)\n# lr_test_pred = lr_model.transform(test_df)\n\n# print roc_eval.evaluate(lr_test_pred)\n\n# print zip(lr_model.getEstimatorParamMaps(), lr_model.validationMetrics)\n\n# display_heat_map(lr_model.validationMetrics, lr_max_iter, lr_reg_param)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(\n  standardization=False, threshold=0.5,\n  predictionCol='LR_P', probabilityCol='LR_Prob', rawPredictionCol='LR_rawProb',\n  maxIter=80, elasticNetParam=0.0, weightCol='weight'\n)\nroc_eval.setRawPredictionCol('LR_rawProb')\n\nlr_models = []\nlr_best_score = 0\nlr_best_model = None\n\nlr_reg_param = [1e-6]\nfor r in lr_reg_param:\n  lr.setRegParam(r)\n  model = lr.fit(train_vec_df)\n  valid_pred_df = model.transform(valid_vec_df)\n  valid_roc_score = roc_eval.evaluate(valid_pred_df)\n  \n  lr_models.append(('regParam: {}'.format(r), valid_roc_score))\n  \n  if valid_roc_score > lr_best_score:\n    lr_best_score = valid_roc_score\n    lr_best_model = model\n\nlr_test_pred_df = lr_best_model.transform(test_vec_df)\nlr_test_roc_score = roc_eval.evaluate(lr_test_pred_df)\n\nprint \"Best model scores: {}\".format(lr_test_roc_score)\nprint lr_models"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["#### 5.1.2: Random forest classifier"],"metadata":{}},{"cell_type":"code","source":["# from pyspark.ml.classification import RandomForestClassifier\n\n# rf_num_trees = [40, 80, 160]\n# rf_max_depth = [4, 8, 16, 30]\n\n# rf = RandomForestClassifier(\n#   predictionCol='RF_P', probabilityCol='RF_Prob', rawPredictionCol='RF_rawProb'\n# )\n# roc_eval.setRawPredictionCol('RF_rawProb')\n\n# rf_pip = Pipeline(stages=[encoder, rf])\n\n# rf_param_grid = (\n#   ParamGridBuilder()\n#   .addGrid(rf.numTrees, rf_num_trees)\n#   .addGrid(rf.maxDepth, rf_max_depth)\n#   .build()\n# )\n\n# rf_tv = TrainValidationSplit(\n#   estimator=rf_pip,\n#   estimatorParamMaps=rf_param_grid,\n#   evaluator=roc_eval,\n#   trainRatio=0.8\n#   )\n\n# rf_model = rf_tv.fit(train_df.drop('p_id'))\n# rf_test_pred = rf_model.transform(test_df)\n\n# print roc_eval.evaluate(rf_test_pred)\n\n# print zip(rf_model.getEstimatorParamMaps(), rf_model.validationMetrics)\n\n# display_heat_map(rf_model.validationMetrics, rf_num_trees, rf_max_depth)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(\n  predictionCol='RF_P', probabilityCol='RF_Prob', rawPredictionCol='RF_rawProb',\n  featureSubsetStrategy='sqrt',\n  numTrees=80\n)\nroc_eval.setRawPredictionCol('RF_rawProb')\n\nrf_models = []\nrf_best_score = 0\nrf_best_model = None\n\nrf_max_depth = [16]\nfor md in rf_max_depth:\n  rf.setMaxDepth(md)\n  model = rf.fit(train_vec_df)\n  valid_pred_df = model.transform(valid_vec_df)\n  valid_roc_score = roc_eval.evaluate(valid_pred_df)\n  \n  rf_models.append(('maxDepth: {}'.format(md), valid_roc_score))\n  \n  if valid_roc_score > rf_best_score:\n    rf_best_score = valid_roc_score\n    rf_best_model = model\n\nrf_test_pred_df = rf_best_model.transform(test_vec_df)\nrf_test_roc_score = roc_eval.evaluate(rf_test_pred_df)\n\nprint \"Best model scores: {}\".format(rf_test_roc_score)\nprint rf_models"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### 5.2: Ensemble\n\nThis section is to help the *one hour time limit* and *limited RAM*. Instead of using a complete training set, we fit multiple models to random subsets of the training set and average them together. This helps us avoid the limitations of the Community Edition.\n\n_the following did not imporve my score much_"],"metadata":{}},{"cell_type":"code","source":["def extract_probabilities(input_df, column_names):\n  input_df_column_names = input_df.schema.names\n  num_col = len(input_df_column_names)\n  column_indices = [input_df_column_names.index(name) for name in column_names]\n  \n  def ep(point):\n    arr = []\n    for i in xrange(num_col):\n      if i in column_indices:\n        arr.append(float(point[i][1]))\n      else:\n        arr.append(point[i])\n        \n    return arr\n    \n  return (\n    input_df\n    .rdd\n    .map(lambda row: ep(row))\n    .toDF(input_df_column_names)\n  )\n\ndef ensemble_fit(classifier, t_df, n=40, s=None, prefix=''):\n  if not s:\n    s = 1.0 / n\n  \n  models = []\n  for i in xrange(n):\n    (\n      classifier\n      .setPredictionCol('{}_o_{}'.format(prefix, i))\n      .setProbabilityCol('{}_p_{}'.format(prefix, i))\n      .setRawPredictionCol('{}_rp_{}'.format(prefix, i))\n    )\n    model = classifier.fit(train_vec_df.sample(False, s))\n    models.append(model)\n    \n  return models\n\ndef ensemble_transform(models, t_df):\n  t_acc_df = t_df\n  for model in models:\n    t_acc_df = model.transform(t_acc_df)\n  return t_acc_df\n\ndef ensemble_evaluate(evaluator, t_df, p_col_names):\n  scores = []\n  for col in p_col_names:\n    evaluator.setRawPredictionCol(col)\n    s = evaluator.evaluate(t_df)\n    scores.append(s)\n  return scores\n\ndef avg_p(input_df, o_col_names):\n  return input_df.withColumn('avg', sum(input_df[col] for col in o_col_names)/ len(o_col_names))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n# from pyspark.ml.classification import LogisticRegression\nfrom pyspark.sql.functions import col\n\nrf = RandomForestClassifier(featureSubsetStrategy='sqrt', numTrees=40)\n# lr = LogisticRegression(standardization=False, threshold=0.5, maxIter=80, elasticNetParam=0.0, weightCol='weight')\n\nn = 10\ns = 0.01\nprefix = 'rf'\n\nmodels = ensemble_fit(rf, train_vec_df, n=n, s=s, prefix=prefix)\ntest_ens_pred_df = ensemble_transform(models, test_vec_df)\nrp_col_n = ['{}_rp_{}'.format(prefix, i) for i in xrange(0, n)]\nscores = ensemble_evaluate(roc_eval, test_ens_pred_df, rp_col_n)\np_col_n = ['{}_p_{}'.format(prefix, i) for i in xrange(0, n)]\ntest_ens_ext_p_df = extract_probabilities(test_ens_pred_df, p_col_n)\nprint(scores)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["ens_df = avg_p(test_ens_ext_p_df, p_col_n).select(col('avg').cast(DoubleType()), 'label')\nroc_eval.setRawPredictionCol('avg')\nprint roc_eval.evaluate(ens_df)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## 6: Create a submission\n\nHere we will use the parameters for the simplest single model that resulted in a decent performance on all the training set to make a submission."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(\n  standardization=False, threshold=0.5, regParam=4e-7, maxIter=80,\n  predictionCol='LR_P', probabilityCol='LR_Prob', rawPredictionCol='LR_rawProb'\n)\n\nlr_pip = Pipeline(stages=[encoder, lr])"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["lr_model = lr_pip.fit(joined_train_people_df.drop('p_id').dropDuplicates())"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["raw_test_df = (\n  sqlContext.read.format('com.databricks.spark.csv')\n  .options(header=True, delimiter=',', inferschema=True)\n  .load(data_root+'/act_test.csv')\n)\n\nraw_test_with_time_feat_df = parse_dates(raw_test_df)\n\nraw_test_renamed_df = (\n  raw_test_with_time_feat_df\n  .toDF(*map(lambda col_name: 't_' + col_name, raw_test_with_time_feat_df.schema.names))\n  .withColumnRenamed('t_people_id', 'p_id').withColumnRenamed('t_outcome', 'label')\n)\n\njoined_test_people_df = (\n  raw_test_renamed_df\n  .join(people_renamed_df, on='p_id', how='inner')\n  .withColumnRenamed('t_activity_id', 'id')\n)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["sub_transformed_df = lr_model.transform(joined_test_people_df)\nsub_prob_df = extract_probabilities(sub_transformed_df, ['LR_Prob'])"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["(\n  sub_prob_df\n  .select(\n    sub_prob_df['id'].alias('activity_id'),\n    sub_prob_df['LR_Prob'].alias('outcome')\n  )\n  .coalesce(1)\n  .write\n  .csv(submission_path, header=True)\n)"],"metadata":{},"outputs":[],"execution_count":56}],"metadata":{"name":"red-hat-kaggle","notebookId":4305734151232919},"nbformat":4,"nbformat_minor":0}
