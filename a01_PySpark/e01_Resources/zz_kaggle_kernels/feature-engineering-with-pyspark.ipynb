{"cells":[{"metadata":{"_uuid":"b596b7626e66900a69ba3486fcf778f098723546"},"cell_type":"markdown","source":"# Essentials of Feature Engineering in pyspark - Part I\n\nData preprocessing in Spark\n\nThe most commanly used data preprocessing techniques in Spark approaches are as follows \n  \n  1) VectorAssembler\n\n  2)Bucketing\n\n3)Scaling and normalization\n\n a) StandardScaler\n\n b) MinMAxScaler\n\n c) MaxAbsScaler\n\n d) Elementwise Product\n\n e) Normalizer\n\n4) Working with categorical features\n\na) StringIndexer\n\nb) Converting Indexed values back to text\n\nc) Indexing in vectors\n\nd) One-hot encoding\n\n5) Text data transformers\n\na) tokenizing text\n\nb) Removing common words\n\nc) Creating word combinations\n\nd) Converting words into numerical representations\n\ne) Tf-Idf\n\nf) Word2Vec\n\n6) Feature Manipulation\n\n7) PCA"},{"metadata":{"trusted":true,"_uuid":"df92dcc44be922f1d0a3dbd3f2aab18a7280a153"},"cell_type":"code","source":"# Initializing a Spark session\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd1662dafde4d4e23fe683b33014bb2de4293815"},"cell_type":"markdown","source":"Downloading the dataset\nFor the purpose of this demonstration we will be using three different datasets\n\n1) retail-data/by-day\n\n2) simple-ml-integers\n\n3) simple-ml\n\n4) simple-ml-scaling\n\nThe datasets can be downloaded from this link https://github.com/databricks/Spark-The-Definitive-Guide"},{"metadata":{"trusted":true,"_uuid":"6aaa577136b4b11b3fbd823ed0f75ea916d0d208"},"cell_type":"code","source":"# # Lets us begin by reading the \"retail-data/by-day\" which is in .csv format\n# sales = spark.read.format(\"csv\") \\ # here we space the format of the file we intend to read\n#         .option(\"header\",\"true\") \\ # setting \"header\" as true will consider the first row as the header of the Dataframe\n#         .option(\"inferSchema\", \"true\") \\ # Spark has its own mechanism to infer the schema which I will leverage at this poit of time\n#         .load(\"/data/retail-data/by-day/*.csv\") \\ # here we specify the path to our csv file(s)\n#         .coalesce(5)\\\n#         .where(\"Description IS NOT NULL\") # We intend to take only those rows in the which the value in the description column is not null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e26de501399ba47fbb7dc5ee123c3199515a461"},"cell_type":"code","source":"# Lets us begin by reading the \"retail-data/by-day\" which is in .csv format and save it into a Spark dataframe named 'sales'\nsales = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"data/retail-data/by-day/*.csv\").coalesce(5).where(\"Description IS NOT NULL\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25c2ba1c2a863184ca3300c94409068ac9e752c3"},"cell_type":"code","source":"# Lets us read the parquet files in \"simple-ml-integers\" and make a Spark dataframe named 'fakeIntDF'\nfakeIntDF=spark.read.parquet(\"/home/spark/DhirajR/Spark/feature_engineering/data/simple-ml-integers\")\n# Lets us read the parquet files in \"simple-ml\" and make a Spark dataframe named 'simpleDF'\nsimpleDF=spark.read.json(r\"/home/spark/DhirajR/Spark/feature_engineering/data/simple-ml\")\n# Lets us read the parquet files in \"simple-ml-scaling\" and make a Spark dataframe named 'scaleDF'\nscaleDF=spark.read.parquet(r\"/home/spark/DhirajR/Spark/feature_engineering/data/simple-ml-scaling\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0420990cd362849f8ae1c5dc2cdb0a8d883f3c9e"},"cell_type":"code","source":"sales.cache()\nsales.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2c5ede3bf9a4325a3d82d036d7d790cb2f251fb"},"cell_type":"code","source":"type(sales)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3ee0158d95a25fab0850fbbbcc307fc514e557e"},"cell_type":"markdown","source":"# Vector assembler\n\nThe vector assembler is basically use to concatenate all the features into a single vector which can be further passed to the estimator or ML algorithm. In order to demo the 'Vector Assembler' we will use the 'fakeIntDF' which we had created in the previous steps."},{"metadata":{"trusted":true,"_uuid":"515f787b26b4c9ecd49f519bfc3231cd0cea3337"},"cell_type":"code","source":"# Let us see what kind of data do we have in 'fakeIntDF'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37837e2b881f6ebcf82ecd7aad655fbf7011f9de"},"cell_type":"code","source":"fakeIntDF.cache()\nfakeIntDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b3918da9da4d5962c006708786e3bcfc758854f"},"cell_type":"code","source":"# Let us import the vector assembler\nfrom pyspark.ml.feature import VectorAssembler\n# Once the Vector assembler is imported we are required to create the object of the same. Here I will create an object anmed va\n# The above result shows that we have three features in 'FakeIntDF' i.e. int1, int2, int3. Let us create the object va so as to combine the three features into a single column named features\nassembler = VectorAssembler(inputCols=[\"int1\", \"int2\", \"int3\"],outputCol=\"features\")\n# Now let us use the transform method to transform our dataset\nassembler.transform(fakeIntDF).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20ec07afdf1450631d1da5ae80f3a71069f655e3"},"cell_type":"markdown","source":"# Bucketing\nBucketing is a most straight forward approach for fro converting the contonuous variables into categorical variable let us understand this with an example straight away\n\nIn pyspark the task of bucketing can be easily accomplished using the Bucketizer class.\n\nFirstly, We shall accomplish the noop task of creating bucket borders. Let us define a list\nbucketBorders =[-1.0, 5.0,10.0,250.0,600.0]\n\nNext, let us create a object of the Bucketizer class. Then we will apply the transform method to our target Dataframe \"dataframe\""},{"metadata":{"trusted":true,"_uuid":"feb02d3abc53d001442db8f4ae5803297d04fe4b"},"cell_type":"code","source":"# Let us create a sample dataframe for demo purpose\n\ndata = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\ndataFrame = spark.createDataFrame(data, [\"features\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ba9471affc8ac93c8f45f1ccc7ff839464d6d5"},"cell_type":"code","source":"from pyspark.ml.feature import Bucketizer\nbucketBorders=[-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n\nbucketer=Bucketizer().setSplits(bucketBorders).setInputCol(\"features\").setOutputCol(\"Buckets\")\nbucketer.transform(dataFrame).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c2927183b02e12bb89c9a2cc26eb114a607096a"},"cell_type":"markdown","source":"# Scaling and normalization\n\nScaling and normalization is another common task that we come across while handling continuous varaibles. It is not always imperative to scale and normalize the features. However, it is highly recommended to scale and normalize the features before applying an ML algorithm in order to avert the risk of an algorithm being insensitive to a certain features.\n\nSpark ML provides us with a class \"StandardScaler\" for easy scaling and normaization of features"},{"metadata":{"trusted":true,"_uuid":"d04f2df4e3f107c2b19f4b3cec219fd9df7bc125"},"cell_type":"code","source":"scaleDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3791562aabae16d2c621f1ea5d9ec1afea8e638a"},"cell_type":"code","source":"from pyspark.ml.feature import StandardScaler\n# Let us create an object of StandardScaler class\nScalerizer=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\nScalerizer.fit(scaleDF).transform(scaleDF).show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c5dfcbcb9072d8e3db2ba12226b427856df873b"},"cell_type":"markdown","source":"# MinMaxScaler\n\nThe StandardScaler standardizes the features with a zero mean and standard deviation of 1. Sometimes, we encounter situations where we need to scale values within a given range (i.e. max and min). For such task Spark ML provdies a MinMaxScaler.\n\nThe StandardScaler and MinMaxScaler share the common soul, the only difference is that we can provide the minimum value and maximum values within which we wish to scale the features.\n\nFor the sake of illustration, let us scale the features in the range 5 to 10."},{"metadata":{"trusted":true,"_uuid":"e5519dfa61dc92e4efa3bb414f06d82bcc82625a"},"cell_type":"code","source":"from pyspark.ml.feature import MinMaxScaler\n# Let us create an object of MinMaxScaler class\nMinMaxScalerizer=MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\").setOutputCol(\"MinMax_Scaled_features\")\nMinMaxScalerizer.fit(scaleDF).transform(scaleDF).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02c88e9260647d2fa5aa305f6a9a323f3dca2f65"},"cell_type":"markdown","source":"# MinAbsScaler\n\nSometimes we need to scalerize features between -1 to 1. The MinAbsScaler does exactly this by dividing the features by the maximum absolute values"},{"metadata":{"trusted":true,"_uuid":"9800fedfef76228064b6a09b4fa198c6d86fc5c7"},"cell_type":"code","source":"from pyspark.ml.feature import MaxAbsScaler\n# Let us create an object of MinAbsScaler class\nMinAbsScalerizer=MaxAbsScaler().setInputCol(\"features\").setOutputCol(\"MinAbs_Scaled_features\")\nMinAbsScalerizer.fit(scaleDF).transform(scaleDF).show(truncate =False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5d6cc534249f7c350e019243065c0e9bf4becf2"},"cell_type":"markdown","source":"# ElementwiseProduct\n\nWhat differentiates ElementwiseProduct from the previously mentioned scalizers is the fact that, in ElementwiseProduct the features are scaled based on a multiplying factor. \n\nThe below mentioned code snippet will transform the feature#1 --> 10 times, feature#2 --> 0.1 times and feature#3 --> -1 times \n\nFor example --> the features [10, 20, 30] if scaled by [10, 0.1, -1] will become [100, 2.0, -30]"},{"metadata":{"trusted":true,"_uuid":"bc7fdce3b39bd6a3b220902b79d56503ec435333"},"cell_type":"code","source":"from pyspark.ml.feature import ElementwiseProduct\nfrom pyspark.ml.linalg import Vectors\n\n# Let us define a scaling vector \n\nScalebyVector=Vectors.dense([10,0.1,-1])\n\n# Let us create an object of the class Elementwise product\nScalingUp=ElementwiseProduct().setScalingVec(ScalebyVector).setInputCol(\"features\").setOutputCol(\"ElementWiseProduct\")\n# Let us transform\nScalingUp.transform(scaleDF).show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4093d122f24ded54c6be2236267e2404d0a3f68"},"cell_type":"markdown","source":"# Normalizer\n\nThe normalizer allows the user to calculate distance between features. The most commonly used distance metircs are \"Manhattan distance\" and the \"Euclidean distance\". The Normalizer takes a parameter \"p\" from the user which represents the power norm.\n\nFor example, Manhatan norm (Mahnatan distance) p = 1; Euclidean norm (Euclidean distance) p = 2;"},{"metadata":{"trusted":true,"_uuid":"13c9f38a8a362355c697712989330bf30764da84"},"cell_type":"code","source":"from pyspark.ml.feature import Normalizer\n# Let us create an object of the class Normalizer product\nl1_norm=Normalizer().setP(1).setInputCol(\"features\").setOutputCol(\"l1_norm\")\nl2_norm=Normalizer().setP(2).setInputCol(\"features\").setOutputCol(\"l2_norm\")\nlinf_norm=Normalizer().setP(float(\"inf\")).setInputCol(\"features\").setOutputCol(\"linf_norm\")\n# Let us transform\nl1_norm.transform(scaleDF).show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a610ab1b73cf7ba89cf8185f10cb1e4ae51b838"},"cell_type":"code","source":"l2_norm.transform(scaleDF).show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac81eb11a23e6977d500bb22e48d11ee85361c79"},"cell_type":"code","source":"linf_norm.transform(scaleDF).show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cdf48db19ac7bc1b12a8d64660a94b6d2a8540f"},"cell_type":"markdown","source":"# StringIndexer (Converting strings to numerical values)\n\nMost of the ML algorithms require converting categorical features into numerical ones. \n\nSparks StringIndexer maps strings nto different numerical values. We will use the simpleDF dataframe for demo purpose which consist of a feature \"lab\" which is a categorical variable.\n"},{"metadata":{"trusted":true,"_uuid":"c82145f2d80a15c3c4f175a6f3f1396fd4d44f31"},"cell_type":"code","source":"simpleDF.show(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d23d1ebb30982aae30ea927de97e6ca47577a817"},"cell_type":"markdown","source":"Let us apply string indexer to a categorical variable named \"lab\" in \"simpleDF\" DataFrame."},{"metadata":{"trusted":true,"_uuid":"21dd2509a721555d46058f6d700765e424c62677"},"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer\n# Let us create an object of the class StringIndexer\nlblindexer=StringIndexer().setInputCol(\"lab\").setOutputCol(\"LabelIndexed\")\n# Let us transform\nidxRes=lblindexer.fit(simpleDF).transform(simpleDF)\nidxRes=idxRes.drop(\"value1\",\"value2\")\nidxRes.show(5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"54a424e263f289f6b7c0be9fc2d2d2bbf307cde9"},"cell_type":"markdown","source":"# IndexToString\n\nSometimes we come accross situations where it is necessary to convert the indexed values back to text. To do this the Spark ML provides a class IndextoString. To demonstrate the \"IndextoString\" let us use the \"LabelIndexed\" column of  \"idxRes\" dataframe which was created in the previous code snippet.\n\nThe LabelIndexed column consists of 1.0 --> good and 0.0 --> bad. Nw let us try and reverse this"},{"metadata":{"trusted":true,"_uuid":"b8cdf70907bf0b87bddb79ac93d048f928afd232"},"cell_type":"code","source":"from pyspark.ml.feature import IndexToString\nLabelReverse=IndexToString().setInputCol(\"LabelIndexed\").setOutputCol(\"ReverseIndex\")\nLabelReverse.transform(idxRes).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b944dde4fcfc959c8b9d9edde02a4df3f4d40e53"},"cell_type":"markdown","source":"# Indexing within Vectors\n\nSpark offer yet another class named \"VectorIndexer\". The \"VectorIndexer\" identifies the categorical variables with a set of features which is already been vectorized and converts it into a categorical feature with zero based category indices.\n\nFor the purpose of illustration let us first create a new DataFrame with features in the form of Vectors."},{"metadata":{"trusted":true,"_uuid":"8317f084fc01f5d8dbc5f55002b3abd4273625e8"},"cell_type":"code","source":"from pyspark.ml.linalg import Vectors\ndataln=spark.createDataFrame([(Vectors.dense(1,2,3),1),(Vectors.dense(2,5,6),2),(Vectors.dense(1,8,9),3)]).toDF(\"features\",\"labels\")\ndataln.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e10552698cc15233852d5c370a0fb15f1b0e7b5"},"cell_type":"code","source":"from pyspark.ml.feature import VectorIndexer\nVecInd=VectorIndexer().setInputCol(\"features\").setMaxCategories(2).setOutputCol(\"indexed\")\nVecInd.fit(dataln).transform(dataln).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b38c404958419ea0863f03b6205a40852fe63f7"},"cell_type":"markdown","source":"# One hot endcoding\n\nOne hot encoder is the most common type of transformation performed during pre-processing. Let us look at an example straight away."},{"metadata":{"trusted":true,"_uuid":"fc37fe47f6555b6bff4691175ce8af814b929643"},"cell_type":"code","source":"simpleDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6afd4bb5355f1badf8a6e50d2c661dcc701ef232"},"cell_type":"code","source":"# Let us encode the \"color\" feature in the \"simpleDF\"\nfrom pyspark.ml.feature import StringIndexer,OneHotEncoder\nSI=StringIndexer().setInputCol('color').setOutputCol('StrIndexed')\nColorIdx=SI.fit(simpleDF).transform(simpleDF)\nohe=OneHotEncoder().setInputCol('StrIndexed').setOutputCol(\"oheIndexed\")\nohe.transform(ColorIdx).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"147987b0d0a8a24bed5c106508d5e1308570c809"},"cell_type":"markdown","source":"# Tokenizing text\n\nTokenizing is the process of converting free form text into a sequence of tokens. Spark ML offers a Tokenizer class to do this task. The \"Description\" column in sales dataframe consists text with words seperated with white spaces. Let us use this for the sake of or demo. "},{"metadata":{"trusted":true,"_uuid":"d33550b583efcbdc99c68d3e6080d624a8be71ed"},"cell_type":"code","source":"# Let us import the tokenizer\nfrom pyspark.ml.feature import Tokenizer\n# Create an object of the Tokenizer class\nTok=Tokenizer().setInputCol(\"Description\").setOutputCol(\"Tokenized\")\nsales_tok=Tok.transform(sales).select(\"Description\",'Tokenized')\nsales_tok.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"589ae0b53afbf3cbf071fa246a34a3061d81e69e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca512fb7651939570941ac0182d67f3ba655dcbb"},"cell_type":"markdown","source":"# RegexTokenizer\n\nTokenizer class by default considers the white space between the words as seperator. However, at times, we may come across various seperators such as '|', '\\' or '@'. To handle such situations, Spark ML provides the RegexTokenizer class.\n\nTo demonstrate this class let us create our own dataframe named \"data_txt\" which consists of text seperated by '|'"},{"metadata":{"trusted":true,"_uuid":"96bd8dbf0828a64d879868668f8a53efb1655002"},"cell_type":"code","source":"from pyspark.sql.types import StringType\nmydata=['Too Fast For You','For|Your|Eyes|Only','As|a|Matter|of|Fact','As|far|as|I|know','Away|from|Keyboard']\ndata_txt=spark.createDataFrame(mydata,StringType()).toDF(\"Text\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0077e64b329bf79de8f95c93ffef65c37a84bd64"},"cell_type":"code","source":"data_txt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eccd7db71bbd89c27be73e1e6caa0588ef89f48"},"cell_type":"code","source":"# Let us import RegexTokenizer class\nfrom pyspark.ml.feature import RegexTokenizer\n# Create an object of this class\nRegTok=RegexTokenizer().setInputCol('Text').setOutputCol(\"Tokenized\").setPattern(\"|\").setGaps(False)\nRegTok.transform(data_txt).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a0b00dd68551a3c584ca1bceaf244eaea4dd50f"},"cell_type":"code","source":"from pyspark.sql.types import StringType\nmydata=['Too Fast For You You','For Your Eyes Only','As a Matter of Fact','As far as I know','Away from Keyboard']\ndataln=spark.createDataFrame(mydata,StringType())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fcffa16d6b042d2ad67211844783f30ebdd7711"},"cell_type":"code","source":"dataln.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"810251df907d4a1939879e7462ba8c8e0959cdc9"},"cell_type":"markdown","source":"# Removing Stopwords\n\nThe common task in NLP after tokenizing is to remove the stopwords such as 'the', 'and', 'but',etc. Spark ML offers StopWordsRemover class to handle this task."},{"metadata":{"trusted":true,"_uuid":"29a9117744d26a6f38e0fddbaad65badbeb2d98d"},"cell_type":"code","source":"# Let us import the StopWordsRemover\nfrom pyspark.ml.feature import StopWordsRemover\n# Let us import a predefined corpus of stopwords\nenglishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n# Create an object of StopWordsRemover\nstops=StopWordsRemover().setStopWords(englishStopWords).setInputCol('Tokenized').setOutputCol(\"Stops_removed\")\nstops.transform(sales_tok).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0bcb2a3f950f1baf61b490e124ddd6c4a937d80"},"cell_type":"markdown","source":"# uni grams, bi grams, tri grams ..... N grams\n\nBig data Processing made simple\n\nThe bigram of the above sentence would look like\n\"Big data\" \"data Processing\" \"Processing made\" \"made simple\"\n\nThe trigram of the above sentence would look like\n\"Big data Processing\" \"data Processing made\" \"Processing made simple\" \n\nAnd so on\n"},{"metadata":{"trusted":true,"_uuid":"4fdd144caf56c15e3c3471c38cfea41c9453e399"},"cell_type":"code","source":"# Let us import the NGram class\n\nfrom pyspark.ml.feature import NGram\nuni_gram=NGram().setInputCol(\"Tokenized\").setOutputCol(\"uni_gram\").setN(1)\nbi_gram=NGram().setInputCol(\"Tokenized\").setOutputCol(\"bi_gram\").setN(2)\ntri_gram=NGram().setInputCol(\"Tokenized\").setOutputCol(\"tri_gram\").setN(3)\n\n# uni_gram.transform(sales_tok.select(\"Tokenized\"))\nbi_gram.transform(sales_tok.select(\"Tokenized\")).show(truncate=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f49d4ec0383365200ae3451c7bf443206cf021c9"},"cell_type":"markdown","source":"# Converting words to numerical representation (CountVectorizer)\n\nThe 'CountVectorizer' does the following:\n\n1) Counts the total number of words in the complete document\n\n2) For each word in each row (or sentence), it counts the the number of ouccurences of that particular word in the entire documnent.\n\n3) For each word in each row (or sentence), it counts the the number of ouccurences of that particular word in the given row (or sentence).\n\n\nIn addition, a word is included in the vocab only if it satisfies the following criteria\n\n1) minTF --> minimum term frequency (the term freq of a word shd be > minTF)\n\n2) minDF --> it is the minimum number of documents (or sentences) a word must appear\n\n3) vocabsize --> total maximum size of vocab."},{"metadata":{"trusted":true,"_uuid":"d29b702f63dfe289046d72ea9e75284ead33e789"},"cell_type":"code","source":"from pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer().setInputCol(\"Tokenized\").setOutputCol(\"CountVec\").setVocabSize(500).setMinTF(1).setMinDF(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce29ff501d60351be7acded92c1b9fed67b2ee4d"},"cell_type":"code","source":"cv.fit(sales_tok).transform(sales_tok).select(\"Tokenized\",\"CountVec\").show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b38e56eeee2fbb01a99f96b62010698da3cef54"},"cell_type":"code","source":"from pyspark.ml.feature import Tokenizer\nTok=Tokenizer().setInputCol(\"value\").setOutputCol(\"Tokenized\")\ndataln=Tok.transform(dataln)\ndataln.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb1744d491489db50b5272b26d43eda3f557cdb4"},"cell_type":"code","source":"from pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer().setInputCol(\"Tokenized\").setOutputCol(\"CountVec\").setVocabSize(500).setMinTF(1).setMinDF(1)\ncv.fit(sales_tok).transform(dataln).select(\"Tokenized\",\"CountVec\").show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e6a193de65ef327e618483dc10869918699c20ab"},"cell_type":"markdown","source":"# TF-IDF (Term frequency to inverse document frequency)\n\nTF-IDF measures how often a word occurs in each document, weighted according to how many documents that word occurs in. In order to demonstrate the TF-IDF in pyspark let us create a new dataframe named tfIdfln. We will derive this dataframe from sales_tok that we had previously created.\n\nFrom the \"Tokenized\" column of the \"sales_tok\" let us keep only those records which contain the word \"red\". This can be done as follows:"},{"metadata":{"trusted":true,"_uuid":"7e3e4d78fce4feba4104f82841047ab723d69bae"},"cell_type":"code","source":"tfIDFln=sales_tok.where(\"array_contains(Tokenized,'red')\").select(\"Tokenized\").limit(10)\ntfIDFln.show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15461cd3bce8e5e90b60b5d4a7f091957e9c3b7"},"cell_type":"markdown","source":"Now let us import the two important classes offered by Spark HashingTF and IDF, creates objects of them and apply transformation"},{"metadata":{"trusted":true,"_uuid":"fe22c1861a923a9ec1c3635e55d943282e7a5c68"},"cell_type":"code","source":"from pyspark.ml.feature import HashingTF,IDF\ntf=HashingTF().setInputCol(\"Tokenized\").setOutputCol(\"TFOut\").setNumFeatures(10000)\nidf=IDF().setInputCol(\"TFOut\").setOutputCol(\"IDFOut\").setMinDocFreq(2)\nidf.fit((tf.transform(tfIDFln))).transform((tf.transform(tfIDFln))).select(\"IDFOut\").show(1,False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"08fe903a6ebd176ac51c91e3cf670ae721174b89"},"cell_type":"markdown","source":"# Word2Vec\n\nWord2Vec is a deep-learning based frame work for computing vector representations of a set of words. The goal is to have similar words close to one another in a vector space.\n\nIn order to demonstrate Word2Vec let us first initialize a new DataFrame named \"documentDF\", it only contains a few sentences."},{"metadata":{"trusted":true,"_uuid":"7f51214629c571476291de224738c0ff995602ab"},"cell_type":"code","source":"documentDF=spark.createDataFrame([(\"Hi I heard about Spark\".split(\" \"), ),(\"I wish Java could use case classes\".split(\" \"), ),(\"Logistic regression models are neat\".split(\" \"), )],[\"text\"])\ndocumentDF.show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f2b712483239c831094647c2e37b2de4109068c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2294e865affeb7e154189ce1a2ee0761a425f436"},"cell_type":"code","source":"from pyspark.ml.feature import Word2Vec\nw2v=Word2Vec(vectorSize=5,minCount=0,inputCol=\"text\",outputCol=\"result\")\nmodel=w2v.fit(documentDF)\nresults=model.transform(documentDF)\n# minCount is the minimum number of times a word should appear in the complete document to be included in the vocabulary\n# fro more information about various parameters in Word2Vec please refer https://spark.apache.org/docs/2.1.1/api/java/org/apache/spark/mllib/feature/Word2Vec.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd88b6c58ab70f95f741eb29cde24da0d60cd67b"},"cell_type":"code","source":"for row in results.collect():\n    text, vector = row\n    print(\"Text:[%s] => \\n Vector: %s\\n\" % (\",\".join(text),str(vector)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70ca715599e5728929d9745a7bc03c8db60c9571"},"cell_type":"markdown","source":"# Principal Component Analysis (PCA)\n\nPCA helps us find the most important aspects of our data. It is a dimensionality reduction technique, which is employed when the dataset consists of a large number of features. PCA changes the feature representation of our data by deriving new features from the original features.\n\nPCA takes a parameter 'k' which specifies the number of output features.\n\nIn order to demonstrate feature selection let us go back to our scaleDF dataframe which we had created earlier. Remenber, it consists of 3 features"},{"metadata":{"trusted":true,"_uuid":"abf48d8b1a0259af7b230061e6ab98cf2dd1542a"},"cell_type":"code","source":"scaleDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91c2fb2ab4fd4712e0a2af25cc942e982ca9a10c"},"cell_type":"code","source":"from pyspark.ml.feature import PCA\npca=PCA().setInputCol(\"features\").setOutputCol(\"PCA_features\").setK(2)\nscaleDF=pca.fit(scaleDF).transform(scaleDF)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9ffda3f799834300620e28a464044f4c683477"},"cell_type":"code","source":"scaleDF.show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24def71c7ea5d91b9bb220bc5379219d9e8cc9b3"},"cell_type":"markdown","source":"# Feature selection\n\nOne of the most commonly used feature selection approach is the Chisquare selector. The Chisquare uses statistical methods to identify features which are significant by determining a p-value corresponding to every feature. Once the p-value is determined the \"best\" features can be selected in one of the three ways: \n\n1) NumTopFeatures: In this case, the user is required to specify the desired number of features (N) to be selected. All the features are then sorted in the ascending order of p-value and to N features are selected.\n\n2) percentile: In this case the user user may specify the percentage of total num of features, and then the features are selected on based on the p-value on lower-the-better-basis\n\n3) threshold (fpr): In this case, we sipmly set threshold over the p-value (mostly 0.05). All features with p-value below the given threshold are considered to be significant and are selected.\n\nLet us start from scratch and develop a code example. We will work with the sales dataframe that we created earlier."},{"metadata":{"trusted":true,"_uuid":"1636a0abea8219d35d178d6fe3d54e8ee107c11a"},"cell_type":"code","source":"sales=sales.where(\"Description is NOT NULL\").where(\"CustomerID is NOT NULL\").select(\"Description\",\"CustomerID\")\nsales.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d32e4c3a9ee6e92a25fc35ad3486db2a76e140e"},"cell_type":"code","source":"# Let us tokenize the \"Description\"\nfrom pyspark.ml.feature import Tokenizer\nTok=Tokenizer().setInputCol(\"Description\").setOutputCol(\"Tokenized\")\nsales=Tok.transform(sales)\nsales.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42d8781978824d8cad52ef29c42d9c9f13774e59"},"cell_type":"code","source":"# Let us countvectorize the Tokenized column\nfrom pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer().setInputCol(\"Tokenized\").setOutputCol(\"CountVec\").setVocabSize(500).setMinTF(1).setMinDF(2)\nsales=cv.fit(sales).transform(sales)\nsales.show(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d003a8f57130e872bc4c1357d5cff69080dfe3e9"},"cell_type":"markdown","source":"Now we will consider the \"CountVec\" as our features and \"CustomerID\" as our labels"},{"metadata":{"trusted":true,"_uuid":"9fd50685825b109f42c37ee87e5fe09ca3fc10ec"},"cell_type":"code","source":"from pyspark.ml.feature import ChiSqSelector\nchisq=ChiSqSelector().setFeaturesCol(\"CountVec\").setLabelCol(\"CustomerID\").setNumTopFeatures(5).setOutputCol(\"Aspects\")\nsales=chisq.fit(sales).transform(sales)\nsales.drop(\"Description\",\"CustomerID\",\"Tokenized\").show(truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c26872ae23d75334007962d7eaa17113313cfc7d"},"cell_type":"markdown","source":"# Polynomial Expansion\n\nPolynomial expansion is used to generate interaction variables of all the input columns. With\npolynomial expansion, we specify to what degree we would like to see various interactions. For\nexample, for a degree-2 polynomial, Spark takes every value in our feature vector, multiplies it\nby every other value in the feature vector, and then stores the results as features. For instance, if\nwe have two input features, we’ll get four output features if we use a second degree polynomial\n(2x2). If we have three input features, we’ll get nine output features (3x3). If we use a thirddegree\npolynomial, we’ll get 27 output features (3x3x3) and so on. This transformation is useful\nwhen you want to see interactions between particular features but aren’t necessarily sure about\nwhich interactions to consider."},{"metadata":{"trusted":true,"_uuid":"bbf962ec5fdda2411f9cfb858164345b1ef71afb"},"cell_type":"code","source":"scaleDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ee035d1625ff6a650d9ca3c6353f6033480ec14"},"cell_type":"code","source":"from pyspark.ml.feature import PolynomialExpansion\nPE=PolynomialExpansion().setInputCol(\"PCA_features\").setOutputCol(\"Poly_features\").setDegree(2)\nscaleDF=PE.transform(scaleDF)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7107b5b82f812da644e48f3b69b6c0f260db4315"},"cell_type":"code","source":"scaleDF.select('PCA_features','Poly_features').show(10,truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8618f3cc8165a872d557761de4f1ae70010869f4"},"cell_type":"markdown","source":"# Conclusion\nIn this humble blog I have tried to cover some basic and widely used data preprocessing transformations offered by Spark ML. I have demonstrated each of them with an illustration. However, there is a plethora of Spark tools for to aid the feature engineering task some of these I will try to cover in my next blog."},{"metadata":{"_uuid":"15904b31e4ebc57a50851a97f3e880e93780fc16"},"cell_type":"markdown","source":"# Bibliography\nChambers, B., & Zaharia, M., 2018. Spark: The definitive guide. \" O'Reilly Media, Inc.\"."},{"metadata":{"trusted":true,"_uuid":"bce2641a37b5308fef428956f40fce62dc459650"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}